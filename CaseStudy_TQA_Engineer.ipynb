{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisatfrogs/bootcamp-results/blob/main/CaseStudy_TQA_Engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Case Study: Evaluation of generated headlines**\n",
        "\n",
        "### **Situation**\n",
        "Company X is a publishing house who has a DPA (Deutsche Presse Agentur) license and wants to use published articles to generate traffic on their website. To do this, they trained a language model to generate headlines given a news article.\n",
        "\n",
        "### **Task**\n",
        "As a text quality engineer, you were hired to check the quality of the generated headlines and in general, to measure the effectivity of the said mechanism they put in place."
      ],
      "metadata": {
        "id": "UHCtvUSsWZ8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1**: How do you define a \"good\" text in this context? (10)"
      ],
      "metadata": {
        "id": "fK6Udcbzgl88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2**: Based on your answer to Q1, how would you measure this? (5)"
      ],
      "metadata": {
        "id": "boo0lEcGgsOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3**: The executives of Company X additionally want to find out if the generated headlines are better than human-written ones. How would you go about this question? (5)"
      ],
      "metadata": {
        "id": "ZRFeve8tg2yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4**: In addition, Company X wants to highlight the human-aspect of their AI solution. For this, a group of 7 experts were asked to rate the generated headlines with respect to certain aspects."
      ],
      "metadata": {
        "id": "Ng9f_sOhhO4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following aspects need to be inspected:\n",
        "\n",
        "*   Spelling and grammar (spelling_and_grammar)\n",
        "*   Content correctness (content_correctness)\n",
        "*   Readability (readability)\n",
        "*   Main message met (main_message_met)\n",
        "\n",
        "\n",
        "\n",
        "**As a text quality engineer, how would you measure these aspects? Are there any aspects among these that are more difficult to measure than others? Why?** (10)\n"
      ],
      "metadata": {
        "id": "yJnUKvJGxE42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load case_study.csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "ljbRFsU98Z_G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"case_study.csv\")"
      ],
      "metadata": {
        "id": "_cxRUY0xIaoL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **T1.1**: Check the distribution of ratings per user (2)"
      ],
      "metadata": {
        "id": "lfY1XUz2Jv9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A grouping function that displays the distribution of ratings per user"
      ],
      "metadata": {
        "id": "ORuyDUipI2A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **T1.2**: Check the distribution of scores for each aspect (2)"
      ],
      "metadata": {
        "id": "UDnE3Mg-KG9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A grouping function that displays the distribution of scores for each aspect"
      ],
      "metadata": {
        "id": "-x23CovFI2sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **T1.3**: Check the distribution of users per story (i.e. how many headlines were rated by how many users) (2)"
      ],
      "metadata": {
        "id": "VHICws2lKe4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A grouping function that displays the distribution of users per story"
      ],
      "metadata": {
        "id": "7FVpYceII3X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **T2**: How do you define the outliers for each criterion? Remove these according to your definition. (3)\n"
      ],
      "metadata": {
        "id": "-S6squEtLY-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "YW7sdttlI4Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **T3**: What is interrater agreement and how do you measure this in terms of this dataset? (5)"
      ],
      "metadata": {
        "id": "So6iSh-rLZIX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56NQTYDGI5Iw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}